\documentclass[oneside,12pt]{report}
\usepackage{enumerate}
%\input 8bitdefs
\hfuzz2pt

%\usepackage[active]{srcltx}
\setlength{\textwidth}{17cm} \setlength{\textheight}{25cm}
 \setlength{\oddsidemargin}{-0.3cm}
 \setlength{\evensidemargin}{1cm}
\addtolength{\headheight}{\baselineskip}
\renewcommand{\baselinestretch}{1.13}
\addtolength{\topmargin}{-3cm}
%\pagestyle{myheadings}
 %\parskip= 1 ex
% % % % % \parindent = 10pt
%%%%%CONTADORES
\newcount\secc %%CONTADOR SECCION
\secc=0
\newcount\sbsecc %%CONTADOR SUBSECCION
\sbsecc=0
\newcount\teor%%%%CONTADOR TEOREMAS
\teor=0
\newcount\prob%%%%CONTADOR PROBLEMAS
\prob=1
\newcount\defi
\defi=0
\newcount\prop
\prop=0
%%%%%%DEFINICIONES DE LAS MACROS
% \def \sp#1{\vskip #1ex} %%SALTO INDENTADO DE #1 lineas
% \def \fsp#1{\vskip #1ex \noindent} %%SALTO NO INDENTADO DE #1 lineas

%%%FORMULAS MATEMATICAS Y SIMBOLOS OPORTUNOS
% \def \va#1{\ #1_{1},\ #1_{2}, \cdots , #1_{n}} %%VARIABLE #1 DE 1 A N
% \def \fun#1#2#3{#1 : #2 \longrightarrow #3} %%FUNCION #1:#2 FLECHA #3
% \def \E{$E\ $}%% PONE E DE MODO MATEMATICO
% \def \suc#1#2{\{ #1_#2 \}}
% \def \su#1#2{#1_#2}
% \def \ep{\varepsilon}
% \def \np{\vskip 0.25 cm}
% \def \ap{\vskip 0.15 cm}
% \def \linf{\lim_{n \rightarrow \infty}}
% \def \limfun#1#2{\lim_{#1 \rightarrow #2}}

\newcommand{\pr}[1]{P( #1 )}

% \def \X{{ X}}
% \def \Y{{\cal Y}}
%\newcommand{\Z}{{\cal Z}}


%%%%CONJUNTOS N, Z, R, Rn,[0,1].vacio
\newcommand{\N}{I\!\!N}
\newcommand{\R}{I\!\!R}
\newcommand{\C}{I\!\!\!\!C}
\newcommand{\Z}{Z\!\!\!Z}
%\def\Zp{\Z^{+}}
\newcommand{\Q}{{\rm{0\!\!\!\!Q}}}
%\def \Rn{\R^{n}}
%\def \int{[0,1]}
%\def \vac{\emptyset}


%%%OTRAS MACROS


% \def \lme#1{\ \> {\hbox{\rm {#1}}}\ \> }%%PONE TEXTO EN MODO MATEMATICO
% \def \lm#1{{\hbox{\rm {#1}}}}%%PONE TEXTO EN MODO MATEMATICO

%%%%FUENTES
% \font\mg = cmr10 scaled \magstep 4 %% FONT NORMAL AUMENTO 4
% \font\g = cmr10 scaled \magstep 2 %% FONT NORMAL AUMENTO 2
% \font\cab=cmr6 %%DEFINICION DE FONT CMR6
% \font\pie=cmr5 %%DEFINICION DE FONT CMR6

%%%% COMEN‚A EL DOCUMENT
%\includeonly{probtema1}
\includeonly{prob1bis,prob2,catalanprob3}
\begin{document}
\parindent = 0pt
\pagestyle{headings}

\newcounter{problema}
\newcommand{\prb}{\addtocounter{problema}{1}
\noindent\vskip 2mm {\textbf{\theproblema ) }}}
\newcommand{\sol}[1]{{\textbf{\footnotetext[\theproblema]{Sol.: #1} }}}

\setcounter{problema}{0}

%\setcounter{problema}{69}

\begin{center}
\textsc{Soluciones problemas del Tema 2}
\end{center}



\prb a) $E(W)=E(X+Y+Z)=E(X)+E(Y)+E(Z)=0$;
$Var(W)=Var(X)+Var(Y)+Var(Z)+2(
Cov(X,Y)+Cov(X,Z)+Cov(Y,Z))=1+1+1+2\left(\frac{1}{4}+0-\frac{1}{4}\right)=3$

b) La esperanza ser\'a la misma. Al ser incorreladas
$Var(W)=Var(X)+Var(Y)+Var(Z)=3$


\prb $E(S_n)=\sum_{i=1}^n E(X_i)= n\mu$.
$Var(S_n)=\sum_{i=1}^n\sum_{j=1}^n Cov(X_i,X_j)=\sigma^2
+\rho\sigma^2 +\sum_{i=2}^{n-1} (\sigma^2 +\rho\sigma^2 +\rho
\sigma^2)+ \sigma^2 +\rho\sigma^2= n\sigma^2+2\rho\sigma^2(n-1).$



\prb $E(S_n)=n\mu$.

 $Var(S_n)=\sum_{i=1}^n\sum_{j=1}^n
Cov(X_i,X_j)=\sum_{i=1}^n\left(\sum_{j=1}^i
Cov(X_i,X_j)+\sum_{j=i+1}^n Cov(X_i,X_j)\right)=\newline
\sum_{i=1}^n\left(\sum_{j=1}^i\sigma^2 \rho^{i-j}+\sum_{j=i+1}^n
\sigma^2 \rho^{j-i}\right)=
\sum_{i=1}^n\sigma^2\left(\sum_{k=0}^{i-1}
\rho^{k}+\sum_{k=1}^{n-i} \rho^{k}\right)=\newline
\sigma^2\sum_{i=1}^n
\left(\frac{1-\rho^i}{1-\rho}+\frac{\rho-\rho^{n-i+1}}{1-\rho}\right)=
\frac{\sigma^2}{1-\rho}\sum_{i=1}^n
\left(1-\rho^i+\rho-\rho^{n-i+1}\right)=\newline
\frac{\sigma^2}{(1-\rho)^2}\left(n-2\rho-n\rho^2+2 \rho^{n+1}\right).$

%\prb Mirando en las tablas de distribuciones tenemos que si  una
%v.a. $\chi_n^2$ sigue una distribuci\'on $\chi^2$ con $n$ grados de
%libertad entonces $f_{\chi_n^2(x)}= \frac{x^{\frac{n-2}{2}}
%e^{-\frac{x}{2}}}{ 2^{\frac{n}{2}}\Gamma(\frac{n}{2})}$ si $x>0$ y
%cero en el resto de casos, siendo su funci\'on caracter\'{\i}stica
%$\Phi_{\chi_n^2}(\omega)= \left(\frac{1}{1- j 2
%\omega}\right)^{\frac{n}{2}}$.
%
%a)  Si $x>0$ tenemos que $F_{\chi_n^2}(x)=P(X_1^2\leq x)=P(
%-\sqrt{x}\leq X\leq
%\sqrt{x})=F_{X_1}(\sqrt{x})-F_{X_1}(-\sqrt{x})=2
%F_{X_1}(\sqrt{x})-1$
%
% Si $x< 0 $ entonces $F_{\chi_n^2}(x)=0$.
%
%
% Por lo tanto $f_{\chi_n^2}(x)=\left\{\begin{array}{ll} 0 & \mbox{si } x< 0\\
% 2 f_{X_1}(\sqrt{x}) \frac{1}{2\sqrt{x}}&\mbox{si }
% x \geq  0\end{array}\right.=\left\{\begin{array}{ll} 0 & \mbox{si } x< 0\\
%\frac{x^{\frac{-1}{2}} e^{-\frac{x}{2}}}{
%2^{\frac{1}{2}}\Gamma(\frac{1}{2})}&\mbox{si }
% x\geq 0\end{array}\right.$
%
% Lo que nos da la distribuci\'on de una $\chi^2$ con 1- g.l.
% (Recordemos que $\Gamma(\frac{1}{2})=\sqrt{\pi}$.)
%
%
%
% b) Como las $X_i$ son independientes tambi\'en lo ser\'an  las $X_i^2$
% para $i=1,\ldots ,n$. Por lo tanto $\chi_n^2$ es la suma de $n$ v.a.
% $\chi_1^2$ independientes.
%
%Entonces  $\Phi_{\sum_{i=1}^n X_i^2}(\omega)=\Pi_{i=1}^n
%\Phi_{\chi_1^2}(\omega) =\left(\left(\frac{1}{1- j 2
%\omega}\right)^{\frac{1}{2}}\right)^n=\left(\frac{1}{1- j 2
%\omega}\right)^{\frac{n}{2}}$.
%
%que es la funci\'on caracter\'{\i}stica de una v.a. $\chi^2$ con n-g.l.
%
%
%
%c) Tenemos que calcular ahora la funci\'on de densidad de
%$T_n=\sqrt{\chi_n^2}$.
%
%Si $t\geq 0$ la funci\'on de distribuci\'on de $T_n$ es
%$F_{T_n}(t)=P(T_n\leq t)=P(\sqrt{\chi_n^2}\leq t)=P(\chi_n^2\leq
%t^2)=F_{\chi_n^2}(t^2)$. Si $t<0$ entonces $F_{T_n}(t)=0$.
%
%Entonces $f_{T_n}(t)=\frac{d}{d t}
%F_{T_n}(t)=\left\{\begin{array}{ll}
%  0 & \mbox{si } t\leq 0 \\
%  f_{\chi_n^2}(t^2) 2 t  & \mbox{si }  t>0
%\end{array}\right.=\left\{\begin{array}{ll}
%  0 & \mbox{si } t\leq 0 \\
%  \frac{t^{n-2} e^{-\frac{t}{2}}}{2^{\frac{n}{2}} \Gamma(\frac{n}{2})} 2 t & \mbox{si } t>0
%\end{array}\right.$
%
%d) Si $n=2$ tenemos que $f_{T_2}(t)=\left\{\begin{array}{ll}
%  0 & \mbox{si } t\leq 0 \\
%t  e^{-\frac{t}{2}}  & \mbox{si } t>0
%\end{array}\right.$
%
%Que es la densidad de una Rayleigh de par\'ametro $\alpha=1$
%(Recordemos que $\Gamma(m+1)=m!$ si $m$ es un entero no negativo).
%
%Notemos que desde este punto de vista la distribuci\'on Rayleigh es
%la  distribuci\'on de la norma de un vector aleatorio normal
%bivariante est\'andar.
%
% \prb Con los datos del problema sabemos que las densidades de $X$
% e $Y$ son
% $f_X(x)=\alpha e^{-\alpha x}$ si $x>0$ y cero en el resto de
% casos y $f_Y(y)=\beta e^{-\beta y}$ si $y>0$ y cero en el resto de
% casos. Las correspondientes funciones caracter\'{\i}sticas son
% $\Phi_X(\omega)=\frac{\alpha}{\alpha-j\omega}$ y
% $\Phi_Y(\omega)=\frac{\beta}{\beta-j\omega}$.
%
%
% Consideraremos que las v.a $X$ e $Y$ son independientes.
%
% a) Bajo estas condiciones sabemos que la funci\'on de densidad de
% $Z=X+Y$ es el producto de convoluci\'on de $f_X$ y $f_Y$.
%
%Entonces si $z>0$ tenemos que
% $$f_Z(z)=(f_X*f_Y)(z)=\int_{-\infty}^{+\infty} f_X(x) f_Y(z-x)
% dx=\int_{0}^{z} \alpha e^{-\alpha x}  \beta e^{-\beta(z- x)}
% dx= \frac{\alpha\beta}{\beta-\alpha} \left(e^{-\alpha z}-e^{-\beta
% z}\right).$$
%
%en caso contrario, es decir si $z\leq 0$, se tiene que $f_Z(z)=0$.
%
%
%
% b) $\Phi_Z(\omega)=\Phi_{X,Y}(\omega,\omega)=\Phi_X(\omega)\Phi_Y(\omega)=
%\frac{\alpha}{\alpha-j\omega}\frac{\beta}{\beta-j\omega}$.
%
%Donde, en la segunda igualdad, hemos utilizado que al ser las
%variables independientes la caracter\'{\i}stica conjunta de $X$ e $Y$
%es igual al producto de las caracter\'{\i}sticas marginales
%
%c) Para recuperar la funci\'on de densidad de $Z$ a partir de
%$\Phi_Z$ tenemos que utilizar la f\'ormula de inversi\'on:
%
%
%$f_Z(x)=\frac{1}{2\pi}\int_{-\infty}^{+\infty} \Phi_Z(x)
%e^{-j\omega x} d \omega=
%\frac{1}{2\pi}\int_{-\infty}^{+\infty}\frac{\alpha}{\alpha-j\omega}\frac{\beta}{\beta-j\omega}
% e^{-j\omega x} d \omega$
%
%
% Podemos descomponer en fracciones simples
% $\frac{\alpha}{\alpha-j\omega}\frac{\beta}{\beta-j\omega}=
% \frac{A}{\alpha-j\omega}+\frac{B}{\beta-j\omega}$
%
% Entonces $ \alpha \beta= (A\alpha+ B\beta) - j (A+B)\omega$ de
% donde $A=-\frac{\alpha\beta}{\alpha-\beta}$ y $B=\frac{\alpha\beta}{\alpha-\beta}$
% y por lo tanto
%
% $$ \frac{\alpha}{\alpha-j\omega}\frac{\beta}{\beta-j\omega}=
% -\frac{\beta}{\alpha-\beta} \frac{\alpha}{\alpha-j\omega}+
% \frac{\alpha}{\alpha-\beta}\frac{\beta}{\beta-j\omega}  $$
%
%Entonces
%
%$f_Z(x)=\frac{1}{2\pi}\int_{-\infty}^{+\infty}
%-\frac{\beta}{\alpha-\beta} \frac{\alpha}{\alpha-j\omega}
%e^{-j\omega x} d \omega  + \frac{1}{2\pi}\int_{-\infty}^{+\infty}
% \frac{\alpha}{\alpha-\beta}\frac{\beta}{\beta-j\omega} e^{-j\omega x} d
% \omega=\newline
% -\frac{\beta}{\alpha-\beta} \left(  \frac{1}{2\pi}\int_{-\infty}^{+\infty}
%\frac{\alpha}{\alpha-j\omega} e^{-j\omega x} d \omega  \right)+
% \frac{\alpha}{\alpha-\beta}\left(\frac{1}{2\pi}\int_{-\infty}^{+\infty}
%\frac{\beta}{\beta-j\omega} e^{-j\omega x} d
% \omega\right)$
%
% Ahora las expresiones encerradas en los par\'entesis anteriores son
% las f\'ormulas de inversi\'on de las funciones caracter\'{\i}sticas de dos
% v.a. exponenciales de par\'ametros $\alpha$ y $\beta$
% respectivamente y por lo tanto si $x>0$
%
%$f_Z(x)=-\frac{\beta}{\alpha-\beta}
%f_X(x)+\frac{\alpha}{\alpha-\beta}
%f_Y(x)=-\frac{\alpha\beta}{\beta-\alpha} \left( e^{-\alpha x}-
%e^{-\beta x}\right)$
%
%Valiendo cero en el resto de casos.
%
%%(Hemos utilizado la linealidad de la transformada  inversa.)
%
%
%\prb  Tenemos que $X$ e  $Y$ son v.a. independientes y $Z=a X+ b
%Y$.
%
%a) $\Phi_Z(\omega)= \Phi_{aX+bY}(\omega)= E\left(e^{j \omega (a X+
%b Y)} \right)=E\left(e^{j \omega a X}e^{j \omega b Y} \right)=
%E\left(e^{j \omega a X}\right)E\left(e^{j \omega b Y}
%\right)=\newline \Phi_X(a\omega) \Phi_Y(b\omega)$
%%=e^{ja\omega}\Phi_X(\omega)e^{j b\omega}\Phi_Y(\omega)$.
%
%b) $E(Z)= \frac{1}{j} \frac{d}{d\omega}
%\Phi_Z(\omega)\mid_{\omega=0}=\frac{1}{j} \frac{d}{d\omega}
%\left(\Phi_X(a\omega)
%\Phi_Y(b\omega)\right)\mid_{\omega=0}=\newline
%\frac{1}{j}\left[{\Phi'}_X(a\omega)
%\Phi_Y(b\omega)+{\Phi}_X(a\omega){\Phi'}_Y(b\omega)\right]_{\omega=0}=
%\frac{1}{j} \left[ a
%{\Phi'}_X(\omega)\mid_{a\omega}\Phi_Y(b\omega)+\Phi_X(a\omega) b
%{\Phi'}_Y(\omega)\mid_{b\omega}\right]_{\omega=0}=\newline
%\frac{1}{j} \left[ a j E(X) + b j E(Y)\right]=a E(X)+ b E(Y)$
%
%resultado que ya conoc\'{\i}amos.
%
%De forma similar:
%
%$E(Z^2)=
% \frac{1}{j^2} \frac{\partial^2}{\partial\omega^2}
%\Phi_Z(\omega)\mid_{\omega=0}=\frac{1}{j^2}
%\frac{\partial^2}{\partial\omega^2} \left(\Phi_X(a\omega)
%\Phi_Y(b\omega)\right)\mid_{\omega=0}=\newline
%\frac{1}{j^2}\left[{\Phi''}_X(a\omega) \Phi_Y(b\omega)+
%2{\Phi'}_X(a\omega){\Phi'}_Y(b\omega)+
%\Phi_X(a\omega){\Phi''}_Y(b\omega) \right]_{\omega=0}=\newline
%\frac{1}{j^2}\left[a^2 {\Phi''}_X(\omega)\mid_{a\omega}
%\Phi_Y(b\omega)+ 2ab {\Phi'}_X(\omega)\mid_{a\omega}
%{\Phi'}_Y(\omega)\mid_{b\omega}+ \Phi_X(a\omega) b^2
%{\Phi''}_Y(\omega)\mid_{b\omega} \right]_{\omega=0}=\newline
%\frac{1}{j^2}\left[a^2 j^2 E(X^2) +2ab j E(X) j E(Y)+ b^2 j^2
%E(Y^2) \right]=a^2 E(X^2)+2 a b E(X) E(Y)+ b^2 E(Y^2)=\newline a^2 E(X^2)
%+ 2 a b E(X) E(Y) + b^2 E(Y^2)$
%
%Ahora haciendo $Var(Z)=E(Z^2)-(E(Z))^2$ obtenemos el resultado.


\prb Tenemos que $X_i$ sigue una ley $B(n_i,p)$ para
$i=1,\ldots,k$.

La funci\'on generadora de probabilidades de una v.a. $X_i$ es
$$G_{X_i}(z)=E(z^{X_i})=\sum_{s=0}^{n_i}
z^k\left(\begin{array}{cc} n \\ s\end{array}\right)
p^s(1-p)^{n_i-s}=\sum_{s=0}^{n_i} \left(\begin{array}{cc} n \\
s\end{array}\right) (z p)^s(1-p)^{n_i-s}=(z p + q)^{n_i}$$

donde $q=1-p$.

Entonces, al ser las $z^{X_i}$ v.a. independientes, tenemos que:

$G_{S_k}(z)=E(z^{S_k})=E(\Pi_{i=1}^k z^{X_i})= \Pi_{i=1}^k E(
z^{X_i})= \Pi_{i=1}^k(z p + q)^{n_i}= (z p + q)^{\sum_{i=1}^k
n_i}$

Pero esta \'ultima funci\'on resulta ser la funci\'on generadora de
probabilidades de una v.a.  $B(\sum_{i=1}^k n_i,p)$ y por lo tanto
$S_k$  es una binomial con esos par\'ametros.

Ya hemos discutido en temas anteriores, de distintas  maneras, que
la suma de binomiales independientes y con la misma probabilidad
de \'exito es otra binomial con la misma probabilidad de \'exito y
n\'umero de repeticiones igual a la suma de las repeticiones de cada
binomial. La explicaci\'on es clara, pues al ser independientes y
del mismo par\'ametro, la suma de ellas cuenta el n\'umero de \'exitos
en todas las repeticiones de la prueba.

\prb  Sean $X_1,\ldots, X_n$ v.a. i.i.d. $Po(\lambda_i)$.
 Intuitivamente $S_n=\sum_{i=1}^n X_i$ ser\'a tambi\'en una v.a.
 $Po(\sum_{i=1}^n \lambda_i)$ pues recordemos que la Poison se
 puede entender, bajo determinadas condiciones, como un cierto l\'{\i}mite de
 una v.a. binomial.

 Para comprobar esta propiedad podemos hacerlo de forma directa o
  utilizando la funci\'on ca\-rac\-te\-r\'{\i}s\-ti\-ca o la generadora de
 probabilidades (ya que es una v.a. discreta  no negativa  que
 toma valores enteros).

 La funci\'on caracter\'{\i}stica de $X_i$  es $\Phi_{X_i}(\omega)=
 e^{\lambda_i(e^{-j\omega}-1)}$ entonces

 $\Phi_{S_n}(\omega)=\Pi_{i=1}^n \Phi_{X_i}(\omega)=
 \Pi_{i=1}^n  e^{\lambda_i(e^{-j\omega}-1)}= e^{\sum_{i=1}^n
 \lambda_i(e^{-j\omega}-1)}= e^{(\sum_{i=1}^n \lambda_i)(e^{-j\omega}-1)}$

 que es la funci\'on caracter\'{\i}stica de una v.a. $Po(\sum_{i=1}^n
 \lambda_i)$.

 Si recurrimos a la funci\'on generadora de probabilidades tenemos
 que

 $G_{X_i}(z)=E(z^{X_i})=\sum_{k\geq 0} z^k
 \frac{\lambda_i^k}{k!} e^{-\lambda_i}=\sum_{k\geq 0}
 \frac{(z\lambda_i)^k}{k!} e^{-\lambda_i}= e^{-\lambda_i}\sum_{k\geq 0}
 \frac{(z\lambda_i)^k}{k!}=e^{-\lambda_i} e^{k\lambda_i}= e^{\lambda_i(z-1)
 }$


 Ahora

$G_{S_n}(z)=E(z^{S_n})= E( \Pi_{i=1}^n z^{X_i})= \Pi_{i=1}^n
E(z^{X_i})=
 \Pi_{i=1}^n e^{\lambda_i(z-1)}= e^{(\sum_{i=1}^n \lambda_i)
 (z-1)}$ que es la funci\'on generadora de probabilidades de una $Po(\sum_{i=1}^n
 \lambda_i)$ y por lo tanto $S_n$ sigue esta ley de pro\-ba\-bi\-li\-dad.

\prb En este problema estudiamos la distribuci\'on de la suma de una
cantidad aleatoria de variables aleatorias i.i.d.

a) $E(S_N|N=n)=E(S_n)= n E(X_1)$ entonces $E(S_N|N)=N E(X_1)$.

b) $E(S_N)=E(E(S_N|N))=E(N E(X_1))=E(N) E(X_1)$.

c) $E(e^{j \omega S_N}|N=n)=E( e^{j\omega
S_n})=\left(\Phi_{X_1}(\omega)\right)^n$ luego $E(e^{j \omega
S_N}|N)=\left(\Phi_{X_1}(\omega)\right)^N$.

d)  $\Phi_{S_N}(\omega)=E(e^{j\omega S_N})= E(E(e^{j\omega
S_N}|N))=
E\left((\Phi_{X_1}(\omega))^N\right)=G_{N}(\Phi_{X_1}(\omega))$.


%\prb Sea $N$=n\'umero de trabajos en una hora, $X$ sigue una ley
%geom\'etrica de par\'ametro $p$ y $X=0,1,\ldots$. Sea $X_i$ la v.a.
%que nos da el  tiempo de ejecuci\'on del $i$-\'esimo trabajo, nos
%dicen que la distribuci\'on de $X_i$ es exponencial de media
%$\frac{1}{\alpha}$ es decir $f_{X_i}(x)=\alpha e^{-\alpha x}$ si
%$x>0$ y tambi\'en nos aseguran que los tiempos de ejecuci\'on de los
%trabajos son independientes.
%
%El tiempo total de ejecuci\'on de los $N$ trabajos llegados en una
%hora ser\'a $S_N=\sum_{i=1}^N X_i$
%
%
%Lo resolveremos de dos formas, utilizando el problema 9 y de
%forma directa.
%
%Primera forma: La funci\'on generadora de probabilidades de $N$ es
%$G_N(z)=\frac{p}{1-qz}$ y la funci\'on caracter\'{\i}stica de cada $X_i$
%es $\Phi_{X_i}(\omega)=\frac{\alpha}{\alpha-j\omega}$. Utilizando
%9d) tenemos que
%
%$\Phi_{S_N}( \omega)=G_N(\Phi_{X_1}(\omega))=\frac{p}{1- q
%\frac{\alpha}{\alpha-j\omega}}=p+\frac{p\alpha
%(1-p)}{p\alpha-j\omega}$
%
%
%
%Entonces
%
%$f_{S_N}(x)=\frac{1}{2\pi} \int_{-\infty}^{+\infty}
%\Phi_{S_N}(\omega) e^{-j\omega x} d \omega= \frac{1}{2\pi}
%\int_{-\infty}^{+\infty} \left(p+\frac{p\alpha
%(1-p)}{p\alpha-j\omega}\right) e^{-j\omega x} d
%\omega=\hfill\break p \cdot\left(\frac{1}{2\pi}
%\int_{-\infty}^{+\infty} e^{- j \omega x} d\omega\right)+(1-p)
%\frac{1}{2\pi} \int_{-\infty}^{+\infty} \frac{p\alpha}{p\alpha - j
%\omega } e^{-j\omega x} d \omega$
%
%
%pero la \'ultima integral corresponde a la trasformada inversa de la
%caracter\'{\i}stica de una exponencial de par\'ametro $p\alpha$ mientras
%que la primera es la delta de Dirac luego tenemos que
%
%$f_{S_N}(x)=\left\{
%  \begin{array}{ll}
%   p \delta(x)+(1-p) p\alpha e^{-p\alpha x}& \mbox{si } x\geq 0 \\
%  0 & \mbox{en otro caso}
%  \end{array}\right.$
%
%
%Notemos que
%
%$P(S_N=0)=P(S_N\leq 0)= \int_{-\infty}^0 f_{S_n}(x) dx = u(0) p= p
%$
%
%donde
%
%$u(x)=\left\{
%  \begin{array}{ll}
%  0 & \mbox{si } x< 0 \\
%  1 & \mbox{si } x \geq 0
%  \end{array}\right.$ es la funci\'on escal\'on  que, por definici\'on de la delta de Dirac, cumple que
%   $u(x)=\int_{-\infty}^x \delta(t) dt$.
%
%   Lo que sucede  es que  $S_N$ es una v.a. mixta que tiene una
%   parte discreta, la correspondiente a $S_N=0$, y una continua,
%   en el resto de los casos. La explicaci\'on es clara pues la
%   probabilidad de que se utilice tiempo cero en la ejecuci\'on de
%   los trabajos es igual a la probabilidad de que no llegue ning\'un
%   trabajo en una hora y por lo tanto la probabilidad de que
%   $S_N=0$ no es nula; la distribuci\'on no es continua en ese punto.
%   En el resto de casos $S_N$ se comporta como una v.a. continua.
%   Tambi\'en podr\'{\i}amos decir que con probabilidad $p$ el n\'umero de
%   trabajos que llegan es cero y por lo tanto el tiempo total de
%   ejecuci\'on es cero, mientras que con probabilidad $1-p$ llega
%   al menos un trabajo y el tiempo total de ejecuci\'on es una
%   exponencial de par\'ametro $p\alpha$
%
%   La resoluci\'on del problema de forma m\'as directa
%   nos ayudar\'a a entender c\'omo es la distribuci\'on de $S_N$.
%
%
%Segunda forma:
%
%Sabemos que la suma de $n$-v.a. exponenciales del mismo par\'ametro
%$\alpha$ e independientes sigue una distribuci\'on $n$-Erlang de
%par\'ametro $\alpha$. Esta distribuci\'on tiene por densidad:
%
%$f_{S_{n}}(x)=\left\{\begin{array}{ll}\frac{\alpha e^{-\alpha x}
%(\alpha x)^{n-1}}{(n-1)!} & \mbox{si } x>0\\ 0 &\mbox{ en el resto
%de casos}\end{array}\right.
%$
%
%
%Sea $x> 0$ entonces:
%
%$F_{S_N}(x)=P(S_N\leq x)=\sum_{k\geq 0} P(S_N\leq x/N=k) P(N=k)=
%P(S_N\leq x) P(N=0) + \sum_{k\geq 1} \left[\left(\int_0^x
%\frac{\alpha e^{-\alpha t} (\alpha t)^{n-1}}{(n-1)!} d t\right)
%q^k p\right]=\newline 1\cdot p + \sum_{k\geq 1}
%\left[\left(\int_0^x\frac{\alpha e^{-\alpha t} (\alpha
%t)^{n-1}}{(n-1)!} d t\right) q^k p\right]$
%
%La suma del tiempo de los servicios ser\'a cero solamente si llegan
%cero clientes es decir:
%
%$F_{S_N}(0)=P(S_N\leq 0)= P(S_0\leq 0) P(N=0)=p$
%
%
%Entonces:
%
%
%$F_{S_N}(x)=\left\{
%  \begin{array}{ll}
%     0 & \mbox{si } x<0 \\
%        p & \mbox{si } x=0 \\
%   1\cdot p + \sum_{k\geq 1}\int_0^x
%\left[\left(\frac{\alpha e^{-\alpha t} (\alpha t)^{k-1}}{(k-1)!} d
%t\right) q^k p\right] &  \mbox{si } x>0
%  \end{array}\right.$
%
%
%
%Que tambi\'en podemos escribir, utilizando la funci\'on $u(x)$, como
%
%
%$F_{S_N}(x)= \left( p + \sum_{k\geq 1}\left[\int_0^x
%\left(\frac{\alpha e^{-\alpha t} (\alpha t)^{k-1}}{(k-1)!} d
%t\right) q^k p\right] \right)\cdot u(x)$
%
%
%Es f\'acil ver (ejercicio) que si tomamos
%
%$f_{S_N}(x)=\left\{
%  \begin{array}{ll}
%   p \delta(x)+(1-p) p\alpha e^{-p\alpha x}& \mbox{si } x\geq 0 \\
%  0 & \mbox{en otro caso}
%  \end{array}\right.$
%
%
%tenemos que $F_{S_N}(x)=\int_{-\infty}^x f_{S_N}(t) dt$
%
%Luego  $f_{S_N}$ ser\'a una funci\'on de ``\textsl{densidad}" de
%$S_N$.

\prb Las  v.a.  $X_i$ para $i=1,\ldots,k$  son i.i.d. y toman
valores enteros positivos luego tienen funci\'on generadora de
probabilidades  $G_{X_i}(z)=E(z^{X_i})$.


a) Por el problema anterior tenemos que $E(S_N|N)=N E(X_1)$ y
entonces

 $E(S_N)=E(E(S_N|N))=E(N E(X_1))=E(X_1) E(N)$

La varianza es $Var(S_N)= E(S_N^2) -(E(S_N))^2$

Para calcularlo utilizaremos que $E(S_N^2)=E(E(S_N^2/N))$

$E(S_N^2|N=n)=E((\sum_{i=1}^N X_i)^2|N=n)=E\left((\sum_{i=1}^n
X_i)^2\right) = Var(\sum_{i=1}^n X_i)+(E(\sum_{i=1}^n X_i))^2= n
Var(X_1) + (n E(X_1) )^2$

Entonces $E(S_N^2|N)= N Var( X_1) + (N E(X_1))^2$

Ahora

$Var(S_N)= E(E(S_N^2|N))- (E(S_N))^2= E(N) Var(X_1) + E(N^2)
(E(X_1))^2- (E(X_1) E(N))^2=\newline E(N) Var(X_1)+ (E(X_1))^2
Var(N)$

b)  Recordemos que si $X$ s una v.a. que toma valores enteros no
negativos su funci\'on generadora de probabilidades es

$G_X(z)=E(Z^X)$ y su funci\'on caracter\'{\i}stica es, como siempre,
$\Phi_X(\omega)=E(e^{j\omega X})$

Entonces $\Phi_X(-j \ln z)=E(e^{-j j (\ln z) N})= E(e^{\ln
z^N})=E(z^N)= G_X(z)$

Por otra parte aplicando el problema 9d) a $S_N$

$\Phi_{S_N}(\omega)=G_N(\Phi_{X_1}(\omega))$

De esta \'ultima propiedad junto con la primera observaci\'on se sigue
que

$G_{S_N}(z)=\Phi_{S_N}(-j\ln z)=G_N(\Phi_X(-j\ln z))=G_N(G_X(z))$



\prb La v.a. $N$ sigue una ley $Po(L)$. La v.a. $X_i$ nos da el
tiempo de ejecuci\'on del trabajo y sabemos que
$P(X_i=2)=P(X_i=6)=\frac{1}{2}$, siendo estos tiempos
independientes para $i=1,2,\ldots$.


Sea $W=\sum_{i=1}^N X_i$ el tiempo total de ejecuci\'on de los $N$
trabajos llegados en una hora.

a) Aplicando resultados de problemas anteriores tenemos que $E(W)=E(N) E(X_1)= L
\frac{1}{2}(3+6))=\frac{9}{2} L$

$Var(W)=E(N) Var(X_1)+ E(X_1)^2 Var(N)=L
(Var(X_1)+E(X_1)^2)=L\cdot E(X_1^2)$

Ahora $ E(X_1^2 )= \frac{1}{2}(3^2+6^2)= \frac{45}{2}$
%%%%%%%$Var(X_1)= \frac{45}{2}+ \left(\frac{9}{2}\right)^2=\frac{90
%%%%%%%+81}{4}=\frac{171}{4}$

Entonces $Var(W)=L \frac{45}{2}$

b)  Por problemas anteriores  sabemos que $G_{W}(z)=G_N(G_Z(z))$

Como $N$ es una $Po(L)$ sabemos que $G_N(z)=E(Z^N)= e^{L (z-1)}$
por otra parte

$G_{X_i}(z)=E(z^{X_1})=  z^3\frac{1}{2}+
z^6\frac{1}{2}=\frac{1}{2}(z^3+z^6)$

Entonces $G_{W}(z)=G_N(G_Z(z))=G_N(\frac{1}{2}(z^3+z^6))= e^{L
(\frac{1}{2}(z^3+z^6)-1)}$


%Una vez calculada la funci\'on generadora  de probabilidades la
%podemos utilizar para calcular $E(W)$ y $Var(W)$ mediante las
%f\'ormulas (transparencia 73)
%
%  $E(X)=G'_{X}(1)$ y $Var(X)=G''_{X}(1)+G'_{X}(1)-(G'_{X}(1))^2$
%
%  Comprobar (como ejercicio) que esto es as\'{\i}. (Los resultados son $G'(1)=L
%  \frac{9}{2}$ y $G''(1)=L \frac{36}{2}+(L\frac{9}{2})^2$)

\prb Tenemos que $N_t$ sigue una ley $Po(\lambda t)$

Recordemos que  una expresi\'on de la  desigualdad de Cheb. es

$P(|X-\mu|\geq a)\leq \frac{\sigma^2}{a^2}$ con $a>0$, $E(X)=\mu$
y $Var(X)=\sigma$.

En nuestro caso $E(N_t)=\lambda t$ y $Var(N_t)=\lambda t$ ya que
es Poisson.

Entonces tomando $a= \varepsilon t$  podemos escribir

$P(|N_t-\lambda t|\geq \varepsilon)=P(|N_t-\lambda t|\geq
\varepsilon t)\leq \frac{\lambda t}{(\varepsilon
t)^2}=\frac{\lambda}{\varepsilon^2 t}$

\prb Sea $A=$ estar a favor de la ley y sea $f_n(A)$ la frecuencia
relativa de individuos a favor de la ley entre n votantes. Sabemos
que $P(A)=0.1$

Por Cheb. Consideremos las v.a. $X_1,\ldots,X_n$ definidas como
sigue

$X_i=\left\{
  \begin{array}{ll}
    0 & \mbox{si el } $i$ \mbox{-\'esimo encuestado no est\'a a favor} \\
    1 &  \mbox{si el } $i$ \mbox{-\'esimo encuestado  est\'a a favor}
  \end{array}\right.$

  Bajo estas condiciones $X_i$ sigue una ley $Ber(0.1)$, supondremos
   independencia entre las res\-pues\-tas (es decir los sujetos de la encuesta no
   se influyen unos a otros, o lo que es lo mismo la encuesta est\'a
   bien hecha).

   Entonces la frecuencia de respuesta a favor en una encuesta de tama\~{n}o $n$
   se puede poner como

   $$f_n(A)=\frac{\sum_{i=1}^n X_i}{n}$$

   y tambi\'en sabemos que $\sum_{i=1}^n X_i$ sigue una ley
   $B(n,0.1)$ por lo tanto $E(\sum_{i=1}^n X_i)=n 0.1$ y $Var(\sum_{i=1}^n
   X_i)= n\cdot 0.1\cdot 0.9$

   Entonces

   $P(|f_n(A)-p|< 0.02)=P(|\sum_{i=1}^n X_i-np|< n 0.02)\geq 1-
   \frac{p q}{n (0.02)^2}=0.95$

   Ahora en la \'ultima igualdad nos pondremos en el peor caso
   $p=q=\frac{1}{2}$;
   que es cuando se alcanza el m\'aximo  de $\frac{pq}{n (0.02)^2}$
   y por lo tanto el m\'{\i}nimo de $1-
   \frac{p q}{n (0.02)^2}$

 Luego por Cheb. y poni\'endonos en el peor caso  se
 tienen que encuestar al menos a $n=12500$ individuos  para obtener la precisi\'on
 deseada en la frecuencia como estimador de la proporci\'on
 poblacional de individuos a favor de la legislaci\'on. Si sabemos
 que $p=0.1$ entonces $n=4500$.


Por T.L.C.:

 Veamos que aplicando el teorema del l\'{\i}mite central podemos
 reducir notablemente el tama\~{n}o de la encuesta.


Con la notaci\'on anterior


$\frac{\sum_{i=1}^n X_i  -np}{\sqrt{npq}}$ tiende tener una
distribuci\'on normal est\'andar  cuando $n\to +\infty$.


Entonces

   $P(|f_n(A)-p|< 0.02)=P(\left|\frac{\sum_{i=1}^n X_i}{n}-p\right|<  0.02)=
   P(\left|\frac{\sum_{i=1}^n X_i-np}{n}\right|< 0.02)=
P(|\sum_{i=1}^n X_i-np|< n 0.02)=
 P(\left|\frac{\sum_{i=1}^n X_i-np}{\sqrt{npq}}\right|< \frac{n
 0.02}{\sqrt{npq}})\approx
 P(|Z|\leq \frac{n
 0.02}{\sqrt{npq}})=2 F_Z(\frac{n(0.002)}{\sqrt{npq}})-1$

 Donde $Z$ es una normal est\'andar y $F_Z$ su funci\'on de distribuci\'on.
  Entonces el $n$ buscado se
 puede calcular desde la ecuaci\'on

$0.95=P(|Z|\leq \frac{n
 0.02}{\sqrt{npq}})= 2 F_Z(\frac{n(0.002)}{\sqrt{npq}})-1$

ahora $F_Z(\frac{n(0.02)}{\sqrt{npq}})= \frac{1.95}{2}=0.975$

Mirando en las tablas distribuci\'on normal est\'andar obtenemos que

$F_Z(1.96)=0.975$ y podemos despejar n de
$\frac{n(0.02)}{\sqrt{npq}}=1.96$  obteni\'endose que

$n= pq \left(\frac{1.96}{0.02}\right)^2$


Ahora si sabemos que $p=0.1$ entonces

$n=864.36$ luego necesitaremos encuestar al menos a $865$
personas.

Si no conocemos $p$ nos pondremos en el peor de los casos que es
en el que la proporci\'on poblacional es $p=0.5$ y entonces
$n=2401$.


Luego en cualquier caso mejoramos una barbaridad la estimaci\'on por
la desigualdad de Cheb.

Notemos que en el esp\'{\i}ritu de este problema est\'an resumidos todos
los problemas que requieran el c\'alculo a priori del tama\~{n}o de una
muestra con el objetivo de estimar la  proporci\'on de individuos de
una poblaci\'on que poseen una determinada caracter\'{\i}stica.

\prb Tenemos una suma de $100$ v.a. $X_i$ i.i.d. con funci\'on de
probabilidad

$f_{X_i}(k)=P(X_i=k)=\left\{
  \begin{array}{ll}
    \frac{1}{6} & \mbox{ si } k=1,2,3,4,5,6 \\
    0 & \mbox{en el resto de casos}
  \end{array}\right.$

para $1=1,\ldots, 100$.

Entonces $E(X_i)=3.5$ y $Var(X_i)= \frac{35}{12}$

por lo tanto si $S_{100}=\sum_{i=1}^{100} X_i$ tenemos que

$E(S_{100})=100\cdot 3.5=350$; al ser independientes
$Var(S_{100})= 100 \frac{35}{12}$ y entonces
$\sigma_{S_100}=\sqrt{100 \frac{35}{12}}$



Nos piden la probabilidad de que $S_{100}$ est\'e entre 300 y
400,utilizando una de las expresiones de la des. de Cheb. tenemos
que

$P(300\leq S_{100}\leq 400)= P(300-350\leq S_{100}-350\leq
400-350)=P(-50\leq S_{100}\leq 50)\geq
1-\frac{100\frac{35}{12}}{50^2}= \frac{53}{60}=0.8833$


Bajo estas condiciones tambi\'en podemos utilizar el T.L.C. y
sabemos que la distribuci\'on de la v.a.

$\frac{S_{100}-E(S_{100})}{\sigma_{S_{100}}}=\frac{S_{100}-350}{\sqrt{100
\frac{35}{12}}}$

se aproxima a una normal est\'andar a medida que n crece, por lo
tanto, (si $Z$ es una v.a. normal est\'andar):


$P(300\leq S_{100}\leq 400)= P(\frac{300-350}{\sqrt{100
\frac{35}{12}}}\leq \frac{S_{100}-350}{\sqrt{100
\frac{35}{12}}}\leq \frac{400-350}{\sqrt{100
\frac{35}{12}}})\approx P(\frac{50}{\sqrt{100 \frac{35}{12}}}\leq
Z\leq \frac{50}{\sqrt{100 \frac{35}{12}}})= 2
F_Z(\frac{50}{\sqrt{100 \frac{35}{12}}})-1= 2 F_Z(2.9277)-1\approx
2 F_Z(2.93)-1= 2 0.9985-1=0.997$


Donde la primera aproximaci\'on ($\approx$) es debida al T.C.L. y la
segunda es la que proviene de las tablas de la normal est\'andar.


Nota: Aunque es un procedimiento casi en desuso  para conseguir
una mejor aproximaci\'on del valor de $F_Z(2.9277)$ se suele
interpolar este valor entre los dos m\'as cercanos. En este caso los
m\'as cercanos, en nuestras tablas, resultan ser:


$F_Z(2.92)=0.9982$ y $F_Z(2.92)=0.9985$

Entonces si consideramos que estamos aproximando la funci\'on $F_Z$
por una poligonal que pasa por los puntos tabulados podemos
interpolar el valor de $F_Z(2.9277)$ (por semejanza de
tri\'angulos), es decir

$$\frac{F_Z(2.9277)-0.9982}{0.9985-0.9982}=\frac{2.9277-2.92}{2.93-2.92}$$


de donde $F_Z(2.9277)=\frac{0.0077 0.0003}{0.01
}+0.9982=0.0000231+0.9982=0.9982231\approx 0.9982$.

Finalmente el resultado ser\'a $2 F_Z(2.9277)-1\approx 2\cdot
0.9982-1=0.9964
$

De todas maneras si deseamos m\'as precisi\'on podemos utilizar el
Mathematica, Excel, Minitab, SPSS, MathLab etc..., o programar un
algoritmo de aproximaci\'on.


\prb  Sea $X$= talla de un individuo seleccionado al azar de la
poblaci\'on. Sabemos que $X$ es normal de media $\mu=170$ cm. y
desviaci\'on t\'{\i}pica $\sigma_X=7$.

Tenemos una m.a. de 140 individuos  cada resultado ser\'a una v.a.
$X_i$ para $i=1,\ldots,n$ distribuidas igual que $X$ e
independientes.

Sea $\overline{X}=\frac{\sum_{i=1}^140 X_i}{140}$ la media
aritm\'etica de las alturas de los 140 individuos. Sabemos que
$\mu_{\overline{X}}= \mu_X=170$ y que
$\sigma_{\overline{X}}=\frac{\sigma_{X}}{\sqrt{140}}=\frac{7}{\sqrt{140}}$

Nos piden calcular
$P(|\overline{X}-\mu_X|<1)=P\left(\left|\frac{\overline{X}-\mu_{X}}
{\frac{\sigma_{X}}{\sqrt{140}}}\right|<
\frac{1}{\frac{\sigma_{X}}{\sqrt{140}}}\right)\approx
P\left(|Z|<\frac{1}{\frac{\sigma_{X}}{\sqrt{140}}}\right)= 2
F_Z(1.69)-1= 2 0.9545-1=0.909$

Donde $\approx$ es debida al T.L.C. y  $Z$ es una v.a. normal
est\'andar.

\prb Sea $f_n(6)$ la frecuencia relativa de $6$ en $n$
lanzamientos de un dado. \newline Sea $X_i=\left\{
  \begin{array}{ll}
    1 & \mbox{si sale un }  6\\
    0 & \mbox{en caso contrario}
  \end{array}
 \right.$


Bajo las condiciones del problema las v.a. $X_i$ son i.i.d.
$Ber(\frac{1}{6})$

Sea $\overline{X}_n=\frac{\sum_{i=1}^n X_i}{n}$ la media
aritm\'etica de los $n$ lanzamientos, es evidente que
$\overline{X}_n=f_n(6)$.

Adem\'as su esperanza y su varianza son

$$E(\overline{X}_n)=\frac{1}{6}\mbox{  y }
Var(\overline{X}_n)=\frac{\frac{1}{6}\frac{5}{6}}{n}=\frac{5}{36
n}$$

Entonces

$P(|f_n(6)-\frac{1}{6}|<0.01)=P\left(\left|\frac{\overline{X}_n-\frac{1}{6}}{\sqrt{\frac{5}{36
n}}}\right|<\frac{0.01}{\sqrt{\frac{5}{36 n}}}\right)\approx
P\left(|Z|< \frac{6\sqrt{n} 0.01}{\sqrt{5}}\right)=2
F_Z(\frac{6\sqrt{n} 0.01}{\sqrt{5}})-1=0.95$


Donde en $\approx$ hemos utilizado el T.L.C.  y $Z$ es una normal
est\'andar.


Ahora tenemos que

$F_Z(\frac{6\sqrt{n} 0.01}{\sqrt{5}})=\frac{1.95}{2}=0.975$

Mirando en las tablas de la distribuci\'on normal obtenemos que
$F_Z(1.96)=0.975$

Entonces

$\frac{6\sqrt{n} 0.01}{\sqrt{5}}=1.96$ de donde $n=\frac{5}{36}
(1.96)^2= 5335.555$.

Por lo tanto, utilizando el T.L.C., el tama\~{n}o de la muestra para
asegurar que la frecuencia muestral difiera de la real en menos de
$0.01$ con una probabilidad del $0.95$ es al menos $n=5336$.




\prb  De forma similar a los problemas anteriores:

$f_n(\mbox{cara})=\overline{X}_n$ con $X_i$ v.a. i.i.d.
$Ber(\frac{1}{2})$

  $P(|f_n(\mbox{cara})-\frac{1}{2}|< 0.01)=P\left(\left|\frac{\overline{X}_n-\frac{1}{2}}{\sqrt{\frac{0.25}{
n}}}\right|<\frac{0.01}{\sqrt{\frac{0.25}{n}}}\right)\approx
P\left(|Z|< \frac{0.01 \sqrt{n}}{\sqrt{0.25}}\right)=2
F_Z(\frac{\sqrt{n} 0.01}{\sqrt{0.25}})-1=0.95$

Entonces  $F_Z(\frac{\sqrt{n} 0.01}{\sqrt{0.25}})=0.975$ mirando
en las tablas $F_Z(1.96)=0.975$

Por lo tanto $\frac{\sqrt{n} 0.01}{\sqrt{0.25}}=1.96$ de donde
$n=9604$.

Por el T.L.C. necesitamos una muestra de al memos tama\~{n}o $n=9694$
para asegurar que la frecuencia muestral de caras diferir\'a de la
frecuencia poblacional en menos de $0.01$ con probabilidad $0.95$


\prb Sea $N_t$= n\'umero de mensajes que llegan al multiplexor en
$t$ segundos. Nos dicen que $N_t$ sigue una distribuci\'on $Po(10
t)$ y nos piden $P(N_{600}>650)$.


Podemos calcularlo directamente pero necesitaremos de Mathematica
u otro programa similar:

 $P(N_{60}>650)= 1- P(N_{60}\leq 650)=1-
\sum_{k=0}^{650} \frac{(600)^k }{k!}
e^{-600}=1-0.979346=0.0206544$



Pero podemos tambi\'en aproximar esta probabilidad de la siguiente
forma

$P(N_{60}>650)= 1- P(N_{60}\leq 650)\approx 1- P\left(Z\leq
\frac{650+0.5-600}{\sqrt{600}}\right)= 1-P\left( Z\leq
\frac{50.5}{\sqrt{600}}\right)= 1-F_Z(2.06165)\approx 1-
F_Z(2.06)=1-0.9803=0.0197$



Donde en la primera $\approx$ hemos utilizado el T.L.C. y la
correcci\'on de continuidad de Fisher y en la segunda hemos
aproximado por el valor m\'as cercano en la tablas de la funci\'on de
 distribuci\'on normal est\'andar.


\prb Sea $N$= n\'umero de errores en una p\'agina, sabemos que sigue
una distribuci\'on $Po(2)$. Entonces $P(N>5)=1-P(N\leq
5)=1-0.9834=0.0166$

Sea $X_i$ la v.a. que vale $1$ si la $i$-\'esima p\'agina tiene m\'as de
$5$ errores y cero en caso contrario. Suponiendo independencia
entre los errores de las distintas p\'aginas tenemos que las v.a.
$X_i$ i.i.d. $Ber(0.0166)$.


Sea $X=\sum_{i=1}^{300} X_i$ el n\'umero total de errores entre las
$300$ p\'aginas, esta v.a. seguir\'a una distribuci\'on $B(300,0.0166)$


Entonces $E(X)=4.98$ , $Var(X)=4.89733$ y $\sigma_{X}=2.21299$.

Nos piden $P(X\geq 1)=1-P(X=0)$


De forma exacta la probabilidad ser\'a


$P(X\geq 1)=1-P(X=0)=1- \left(
  \begin{array}{c}
   300 \\
   0
  \end{array}
\right) (1-0.0166)^{300}=0.993407$



Aproximando por una Poisson de media $300\cdot 0.0166$

$P(X\geq 1)=1-P(X=0)\approx 1- \frac{(300\cdot 0.0166)^0}{0!}
e^{-300 0.0166}= 1- e^{-4.98}=0.993126$



Aplicando T.L.C. con correcci\'on de continuidad de Fisher

$P(X=0)\approx P\left(\frac{-0.5-4.98}{2.21299}\leq Z\leq
\frac{0.5-4.98}{2.21299}\right)= F_Z(-2.02441)-F_Z(-2.47629)=
1-F_Z(2.02441)-1+F_Z(2.4629)= -F_Z(2.02441)+F_Z(2.4629)\approx
-F_Z(2.02)+F_Z(2.48)=-0.9783+0.9934=0.0151$

Luego la probabilidad buscada es aproximadamente $P(X\geq
1)=1-0.0151=0.9849$



\prb  Tenemos que $P(\mbox{Error})=0.15$. Sea $X=$n\'umero de
errores en la transmisi\'on de 100 bits (supuestamente transmitidos
de forma independiente). Bajo estas condiciones $X$ sigue una ley
$B(100,0.15)$.

Nos piden $P(X\leq 20)$.


Aproximando por una $Po(15)$ tenemos que

$P(X\leq 20)\approx e^{-15} \sum_{k=0}^{20} \frac{15^k}{k!}=0.917$

Donde la \'ultima igualdad se calcul\'o con el Mathematica ya que
nuestras tablas no llegan a $\lambda=15$ (otras tablas llegan).


Aproximando por una normal tenemos que

$P(X\leq 20)\approx P\left(Z\leq \frac{20+0.5-15}{\sqrt{15
(1-0.15)}}\right)=P(Z\leq\frac{5.5}{3.5701})=F_Z(1.54031)\approx
F_Z(1.54)=0.9382$


\prb Sea $R_i$ los 100 n\'umeros reales entonces $R_i=X_i+E_i$ donde
las $E_i$  son v.a. i.i.d. $U(-0.5,0.5)$.


Entonces $\sum_{i=1}^{100} R_i=\sum_{i=1}^{100}
(X_i+E_i)=\sum_{i=1}^{100} X_i -\sum_{i=1}^{100} E_i$

Luego el error total es $|\sum_{i=1}^{100} E_i|$. Por otra parte

$E(\sum_{i=1}^{100} E_i)=100\cdot 0=0$ y $Var(\sum_{i=1}^{100}
E_i)= 100\cdot\frac{1}{12}$ y su desviaci\'on t\'{\i}pica es
$\sqrt{\frac{100}{12}}$.

Por \'ultimo la probabilidad que se pide es

$P(|\sum_{i=1}^{100} E_i|>6)=1- P(|\sum_{i=1}^{100} E_i|\leq
6)=1-P( -6\leq \sum_{i=1}^{100} E_i\leq 6)\approx\newline
1-P\left(\frac{-6-0}{\sqrt{\frac{100}{12}} }\leq Z\leq\frac{6-0}{
\sqrt{\frac{100}{12}}}\right)=1-(2
F_Z(2.08)-1)=2(1-0.9812)=0.0376$

Para la aproximaci\'on hemos utilizado el T.L.C.

\prb Nos dicen que el periodo de vida \'util de una bater\'{\i}a del
radiofaro es una v.a. $T$ que sigue una ley exponencial con
$E(T)=1$ mes. Sea $T_i$ el tiempo de duraci\'on de la $i$-\'esima
bater\'{\i}a. Suponemos que las duraciones de las bater\'{\i}as son v.a.
i.i.d.

El tiempo de vida de $n$ bater\'{\i}as ser\'a $X_n=\sum_{i=1}^n T_i$,
entonces $E(X_n)=n$ y $Var(X_n)=n$. Nos piden el n\'umero de
bater\'{\i}as $n$ necesario para asegurar que $P(X_n>12)=0.99$.

Luego $0.99=P(X_n>12)=1-P(X_n\leq 12)\approx
1-F_Z(\frac{12-n}{\sqrt{n}})$

Donde en $\approx$ hemos utilizado el T.L.C.

Ahora $F_Z(\frac{12-n}{\sqrt{n}})=1-0.99=0.01$ mirando en las
tablas de la distribuci\'on normal est\'andar tenemos que
$F_Z(2.33)=0.9901\approx0.99$ y por lo tanto $F_Z(-2.33)\approx
0.01$. As\'{\i} que tenemos que resolver la ecuaci\'on

$$\frac{12-n}{\sqrt{n}}=-2.33$$


Notemos que si $n$ es soluci\'on de la ecuaci\'on anterior entonces
$12-n<0$. Resolviendo la ecuaci\'on
$\left(\frac{12-n}{\sqrt{n}}\right)^2=(-2.33)^2$ Las soluciones
son $n=23.23$ y $n=6.19887$; desechamos  esta \'ultima por ser
inferior a $12$. Por lo tanto la aproximaci\'on por el T.L.C. nos
asegura que necesitamos al menos $24$ bater\'{\i}as para asegurar que
el radiofaro tendr\'a energ\'{\i}a para un a\~{n}o con probabilidad $0.99$

\prb Nos piden  si una moneda de la que se obtuvieron 447 caras en
1000 lanzamientos puede considerarse regular, es decir, que su
probabilidad de cara es $0.5$.


Sea $X$=n\'umero de caras obtenido en 1000 lanzamientos de esta
moneda. La distribuci\'on de $X$ ser\'a una $B(1000,p)$ donde $p$ es la
probabilidad de cara.

Entonces

$P(X\leq 447)\approx F_Z(\frac{447+0.5-p 1000}{\sqrt{1000 p(1-p)}})$


Supongamos que $p=0.5$, entonces

$P(X\leq 477/p=0.5)=F_Z(-3.32039)=1-F_Z(3.32039)\approx
1-F_Z(3.32)=1-0.9995=0.0005$


Es decir, si $p=0.5$, es muy raro que esto suceda. Solamente $5$
de cada $10000$ veces que lancemos esta moneda en $1000$ ocasiones
resultar\'a  un n\'umero de caras igual o inferior a $447$.


Si alguien quiere saber t\'ecnicas m\'as precisas para la resoluci\'on
de esta problema tendr\'a que estudiar lo que se llama contraste de
hip\'otesis estad\'{\i}stico. Un buen resumen se encuentra en [1] de la
bibliograf\'{\i}a complementaria.

 \prb Es similar a los anteriores.
 \prb Es similar a los anteriores.

\prb Sea $N$=n\'umero de clientes que llegan en un periodo de
tiempo, nos dicen que $N$ sigue una ley $Po(\lambda)$. Cuando
llega un cliente tiene una probabilidad $p$ de recibir servicio.
Este evento queda modelizado para el $i$-\'esimo cliente por la v.a.

$$X_i=\left\{
  \begin{array}{ll}
   1 & \mbox{si recibe servicio} \\
    0 & \mbox{en caso contrario}
  \end{array}
\right.$$

As\'{\i} la variable que cuenta el n\'umero de usuarios que reciben
servicio es $S_N=\sum_{i=1}^N X_i$ que es la suma de un n\'umero
aleatorio de variables aleatorias. Se nos pide que determinemos la
distribuci\'on de $S_N$.

Por el problema 11b) tenemos que, bajo estas condiciones,
$G_{S_N}=G_N(G_{X_{1}}(z))$.

Sabemos que la funci\'on generadora de probabilidades de una v.a.
$N$ con distribuci\'on $Po(\lambda)$ es $G_N(z)=e^{\lambda(z-1)}$ y
la de una v.a. $X_1$ con distribuci\'on $Ber(p)$ es $G_{X_1}(z)=q+p
z$. Entonces

$G_{S_N}(z)=G_N(G_{X_1}(z))=e^{\lambda(q+p z-1)}= e^{\lambda
p(z-1)}$

que resulta ser la funci\'on generadora de probabilidades de una
$Po(\lambda p)$. Por lo tanto $S_N$ sigue una distribuci\'on
$Po(\lambda p)$.


Tambi\'en podemos resolver el problema de forma directa. Si $k$ es
un entero no negativo tenemos que $P(S_N=k)=\sum_{n\leq 0}
P(S_n=k|N=n)P(N=n)=\sum_{n\leq k} P(S_n=k|N=n)P(N=n)= \sum_{n\leq
k} \left(\begin{array}{c}n\\k\end{array}\right) p^k q^{n-k}
\frac{\lambda^n}{n!} e^{-\lambda}=\frac{(\lambda p)^k}{k!}
e^{-\lambda p}$

Que es la funci\'on de probabilidad de una v.a. con distribuci\'on
$Po(\lambda)$, los detalles de la \'ultima suma se dejan al lector.

%\prb Este problema y los siguientes nos ayudan a calcular lo que
%se llama frecuencia (media) a largo plazo.
%
%
%Sea $S_{N(t)}=\sum_{i=1}^N(t)$= tiempo transcurrido hasta el fallo
%de los $N(t)$ dispositivos instalados en el intervalo de tiempo
%$(0,t]$. En primer lugar notemos que $S_{N(t)}\leq t$ y que
%$S_{N(t)+1}< t$, luego tenemos que $S_{N(t)}\leq t< S_{N(t)+1}$.
%Tambi\'en es obvio que $N(t)>0$.
%
%a) Utilizando las desigualdades anteriores
%
%$$\frac{S_{N(t)}}{N(t)}\leq\frac{t}{N(t)}<
%\frac{S_{N(t)+1}}{N(t)}$$
%
%como se nos ped\'{\i}a comprobar.
%
%b)
%$\lim_{t\to+\infty}\frac{S_{N(t)}}{N(t)}=\lim_{t\to+\infty}\frac{1}{N(t)}
%\sum_{j=1}^{N(t)} X_j$
%
%Por otra parte
%
%$\lim_{t\to+\infty}\frac{S_{N(t)+1}}{N(t)}=
%\lim_{t\to+\infty}\frac{S_{N(t)+1}}{N(t)+1}\frac{N(t)+1}{N(t)}=
%\lim_{t\to+\infty}\frac{S_{N(t)+1}}{N(t)+1}$ ya que
%$\lim_{t\to+\infty}\frac{N(t)+1}{N(t)}=1$
%
%
% Ahora aplicando la Ley
%Fuerte de los Grandes N\'umeros tenemos que
%
%$P(\lim_{t\to+\infty}\frac{1}{N(t)} \sum_{j=1}^{N(t)} X_j=E(X))=
%P(\lim_{t\to+\infty}\frac{1}{N(t)+1} \sum_{j=1}^{N(t)+1}
%X_j=E(X))=1$
%
%Y por lo tanto, con probabilidad 1
%
%$\lim_{t\to+\infty}\frac{S_{N(t)}}{N(t)}=\lim_{t\to+\infty}\frac{S_{N(t)+1}}{N(t)}=E(X)$
%
%Ahora por el apartado  a) deducimos que
%
%$P(\lim_{t\to+\infty}\frac{1}{E(X_1)}\leq
%\lim_{t\to+\infty}\frac{N(t)}{t}\leq
%\lim_{t\to+\infty}\frac{1}{E(X_1)})=1$
%
%y por lo tanto podemos afirmar que la frecuencia de fallo a largo
%plazo es $\frac{1}{E(X)}$ con probabilidad $1$; o de  manera m\'as
%formal (y general)  que si los tiempos de duraci\'on de los
%componentes son i.i.d. como $X_1$ tenemos que
%$\lim_{t\to+\infty}\frac{N(t)}{t}=\frac{1}{E(X_1)}$
%
%
%
%
%
%
%
%\prb Es la aplicaci\'on del problema anterior al caso en que las
%llegadas entre clientes sean v.a. i.i.d. como una v.a. exponencial
%de media $\frac{1}{\alpha}$. Entonces la frecuencia de llegadas a
%largo plazo es con probabilidad 1 $\lim_{t\to
%+\infty}\frac{N(t)}{t}=\frac{1}{E(X_1)}=\alpha$.
%
%
%\prb Al contrario que en el problema 28 el componente que falla en
%el sistema no se reemplaza instant\'aneamente, sino que se tarda una
%cantidad aleatoria de tiempo en repararlo. Este ejercicio resuelve
%en general el c\'alculo de la frecuencia a largo plazo de un ciclo
%de funcionamiento/reparaci\'on de un sistema en el que los tiempos
%de funcionamiento y reparaci\'on sean i.i.d.
%
%
%Sea $U_j=$ tiempo de funcionamiento desde la $j$-\'esima parada que
%son v.a. i.i.d.  igual que la v.a. $U$ para $j=1,2,\ldots$.
%
%Sea $D_j=$ tiempo que permanece el sistema parado desde la
%$j$-\'esima aver\'{\i}a (es decir el tiempo de reparaci\'on de la $j$-\'esima
%aver\'{\i}a) que son v.a. i.i.d.  igual que la v.a. $D$ para
%$j=1,2,\ldots$.
%
%Sea $X_j=U_j+D_j$ es decir tiempo de un ciclo de reparaci\'on que
%tiene la misma distribuci\'on que la v.a. $X=U+D$ para
%$j=1,2,\ldots$..
%
% $N(t)$=n\'umero de paradas en $(0,t]$.
%Suponemos todas las independencias necesarias.
%
%a) $E(X_j)=E(U_j)+E(D_j)=E(U)+E(D)$
%
%b) Sea $S_{N(t)}=\sum_{j=1}^{N(t)} X_j$ la suma de $N(t)$  los
%ciclos de reparaci\'on acaecidos en el intervalo $(0,t]$. Por el
%problema 28b)
%
%
%$$\lim_{t\to +\infty}
%\frac{N(t)}{t}=\frac{1}{E(X)}=\frac{1}{E(U)+E(D)}$$
%
%
%
% \prb Este problema resuelve en general el c\'alculo de costes
% medios
%de reparaci\'on a largo plazo para una situaci\'on similar a los
%problemas anteriores.
%
%$X_j$= tiempo entre el  suceso $j$-\'esimo y el anterior, nos dicen
% que son v.a. i.i.d. como la v.a. $X$ para $j=1,2,\ldots $.
%
%$C_j=$ coste asociado al $j$-\'esimo evento, nos dicen
% que son v.a. i.i.d. como la v.a. $C$ para $j=1,2,\ldots $. Adem\'as
% se asegura que los
%vectores aleatorios $(X_j,C_j)$ son i.i.d.
%
%$N(t)=$ n\'umero de eventos en $(0,t]$
%
%$C(t)=\sum_{j=1}^{N(t)} C_j=$ coste total de los eventos ocurridos
%en $(0,t]$.
%
%a) La expresi\'on $\frac{C(t)}{N(t)}=\frac{\sum_{j=1}^{N(t)}
%C_j}{N(t)}$ es la media aritm\'etica de los costes de los eventos
%ocurridos en $(0,t]$.
%
%
%Entonces por la ley fuerte de los grandes n\'umeros con probabilidad
%$1$ tendremos que
%
%
%$$\lim_{t\to +\infty} \frac{C(t)}{N(t)}=\lim_{t\to
%+\infty}\frac{\sum_{j=1}^{N(t)} C_j}{N(t)}=E(C)$$
%
%
%b) Podemos escribir
%$\frac{C(t)}{t}=\frac{C(t)}{N(t)}\frac{N(t)}{t}$
%
%Entonces, utilizando el problema 28, con probabilidad $1$
%
%
%$\lim_{t\to +\infty} \frac{C(t)}{t}=\lim_{t\to +\infty }
%\frac{C(t)}{N(t)}\frac{N(t)}{t}=  \frac{E(C)}{E(X)}$
%
%
%\prb a) Si $t$ es un instante donde el ciclo se acaba
%
%$\int_{0}^t I_U(t') dt'= \sum_{j=1}^{N(t)}\int_0^{U_j} 1
%dt'=\sum_{j=1}^{N(t)}U_j$
%
%y de aqu\'{\i}
%
%$\frac{1}{t}\sum_{j=1}^{N(t)} U_{j}=\frac{1}{t}\int_{0}^{t}
%I_{U}(t')dt'$
%
%b) Recordemos de los problemas anteriores que un ciclo de
%reparaci\'on  es $X_j=U_j+D_j$ y que $E(X)=E(U)+E(D)$. Si el coste
%de reparaci\'on lo definimos como el tiempo de funcionamiento de
%cada ciclo de reparaci\'on tenemos que
%
% $C(t)=\sum_{j=1}^{N(t)} U_j$ luego el tiempo de funcionamiento medio a
% largo plazo ( coste a largo plazo) es  con probabilidad 1
%
% $\lim_{t \to+\infty} \frac{C(t)}{t}=\lim_{t \to+\infty} \frac{C(t)}{N(t)}
% \frac{N(t)}{t}=\frac{E(U)}{E(X)}=\frac{E(U)}{E(U)+E(D)}$
%
%
% c) De forma an\'aloga si el coste de cada reparaci\'on son v.a.
% i.i.d. $C_j$ como una v.a. $C$, entonces con probabilidad $1$ coste medio de reparaci\'on a
% largo plazo es $\frac{E(C)}{E(U)+E(D)}$
%
%
%\prb  Sea $X_i$= tiempo transcurrido entre la llegada del
%$i$-\'esimo viajero y el anterior. Nos dicen que las v.a. $X_i$ son
%v.a. i.i.d. exponenciales con $E(X_i)=T$, es decir, son
%$Exp(\frac{1}{T})$
%
%Sea $N(t)$= n\'umero de viajeros que llegan en intervalo $(0,t]$,
%del que sabemos que sigue una ley $Po(T)$ (ya que si el tiempo
%entre eventos es exponencial el n\'umero de eventos es Poisson).
%
%Los autobuses salen de la estaci\'on cuando tienen al $m$ pasajeros.
%
%Entonces el n\'umero medio de autobuses que salen de la estaci\'on en
%es $\frac{N(t)}{m t}$ por lo tanto aplicando el problema 28
%tenemos que con pro\-ba\-bi\-li\-dad 1 la frecuencia de salidas  a
%largo plazo es
%
%$$\lim_{t\to+\infty}\frac{N(t)}{m t}=\frac{1}{m
%}\lim_{t\to+\infty}\frac{N(t)}{t}= \frac{1}{m E(X)}=\frac{1}{m
%T}$$
%
%
%\prb Sea $C_j$= duraci\'on del $j$-\'esimo periodo de funcionamiento
%correcto.
%
%Sea $I_j$= duraci\'on del $j$-\'esimo periodo de funcionamiento
%incorrecto.
%
%As\'{\i} $C_j+I_j$= duraci\'on del $j$-\'esimo ciclo de funcionamiento
%correcto/incorrecto. Ahora aplicando el problema 31) tenemos que
%con probabilidad $1$ la proporci\'on de funcionamiento correcto a
%largo plazo es $\frac{C}{E(C)+E(I)}=\frac{\mu_1}{\mu_1+\mu_2}$
%
%
%\prb Sean
%\begin{itemize}
%\item[---] Sea $U_j$= tiempo  que trabaja en el $j$-\'esimo periodo de
%presencia de su jefe
%\item[---] Sea $D_j$= tiempo  que trabaja en el $j$-\'esimo periodo de
%ausencia de su jefe
%\item[---] Sea $X_j$= duraci\'on del  $j$-\'esimo periodo de
%presencia del  jefe
%\item[---] Sea $Y_j$= duraci\'on del  $j$-\'esimo periodo de
%ausencia del  jefe
%\end{itemize}
%
%Las v.a. $X_j$ son i.i.d. como una v.a. $X$ $Exp(\frac{1}{\mu_1})$
%y las  v.a. $Y_j$ son i.i.d. como una v.a. $Y$
%$Exp(\frac{1}{\mu_2})$. Nos dan las relaciones
%
% $U_j=r_1 X_j$ y $D_j= r_2 Y_j$
%
% Entonces $E(U_j)=r_1 \mu_1$ y $ E(D_j)=r_2 \mu_2$, con esto
% tenemos todos los datos y condiciones para aplicar el problema
% 30 y por lo tanto la proporci\'on de tiempo de trabajo a largo
% plazo es, con probabilidad $1$, $\frac{E(U)+E(D)}{E(X)+E(Y)}=
% \frac{r_1\mu_1+r_2\mu_2}{\mu_1+\mu_2}$.
%
%
%
%\prb Sean
%\begin{itemize}
%\item[---] Sea $Y_j$= duraci\'on de la $j$-\'esima llamada, que son v.a.
%i.i.d. como la v.a. $Y$.
%\item[---] Sea $X_k$= tiempo  entre la llegada del $k$-\'esimo individuo, que son v.a.
%i.i.d. como una v.a. $X$ $Exp(\lambda)$.
%\item[---] Sea $N(t)$=n\'umero de clientes que llegan en $(0,t]$ que
%sigue una ley $Po(\lambda t)$.
%\item[---] Sea $U(t)$=n\'umero de clientes que llaman en  $(0,t]$.
%\item[---] Sea $C_j$=n\'umero de clientes que llegan mientras se est\'a realizando la  $j$-\'esima
%llamada. Las $C_j$ son i.i.d. y tienen distribuci\'on $Po(\lambda
%Y_j)$ entonces $E(C_j/Y_j=y_j)= \lambda y_j$ y por lo tanto
%$E(C_j/Y_j)=\lambda Y_j$. Ahora sabemos que
%$E(C_j)=E(E(C_j/Y_j))=E(\lambda Y_j)= \lambda E(Y)$
%\item[---] Sea $V(t)=\sum_{j=1}^{U(t)} C_j$=n\'umero de clientes que se van sin llamar en  $(0,t]$.
%\end{itemize}
%
%
%a) Tenemos las siguientes relaciones
%
%$N(t)=U(t)+V(t)$;
%
%$\frac{N(t)}{t}=\frac{U(t)}{t}+\frac{\sum_{j=1}^{U(t)} C_j}{t}=
%\frac{U(t)}{t}+\frac{\sum_{j=1}^{U(t)} C_j}{U(t)}\frac{U(t)}{t}$.
%
%Entonces  $\lim_{t\to +\infty}\frac{N(t)}{t}=\lim_{t\to +\infty}
%\frac{U(t)}{t}+\lim_{t\to +\infty}\frac{\sum_{j=1}^{U(t)}
%C_j}{U(t)}\lim_{t\to +\infty}\frac{U(t)}{t}$.
%
%Y por lo tanto con probabilidad 1 tenemos que
%
%$$\lambda =r +E(C) r= r + \lambda E(Y) r$$
%
%Despejando obtenemos que la proporci\'on de utilizaci\'on del tel\'efono
%a largo plazo es, con probabilidad 1, $r= \frac{\lambda}{1+\lambda
%E(Y)}$.
%
%b) Nos piden la proporci\'on de clientes a largo plazo que se van
%sin llamar, esto es
%
%$\lim_{t\to+\infty} \frac{V(t)}{N(t)}= \lim_{t\to+\infty}
%\frac{N(t)-U(t)}{N(t)}=1 -\lim_{t\to+\infty} \frac{U(t)}{N(t)}=
%\lim_{t\to+\infty} \frac{U(t)}{t}
%\frac{t}{N(t)}=1-r\frac{1}{\lambda}=\frac{\lambda-r}{\lambda}$
%
%
%
%Luego la proporci\'on de clientes largo plazo que se van sin llamar
%es , con probabilidad $1$, $\frac{\lambda-r}{\lambda}$.
%
%\prb  a) La probabilidad de que llegue un 1 es $P(1)=0.1$ por lo
%tanto  $P(0)=0.9$
%
%Ahora $P(01)=0.09=0.9\cdot 0.1=P(0)P(1)$ luego son independientes.
%De hecho las cadenas que acaban en 1 tendr\'an una longitud que
%vendr\'a dada por una v.a. $X$ con distribuci\'on $Ge(0.1)$ con
%$X=1,2,\ldots$.
%
%Por otra parte  $P(0000)=P(X\geq 4)=0.9^4=0.6561$ ( se deja como
%ejercicio que comprob\'eis las restantes probabilidades).
%
%b)Sea $L_j$= longitud de entrada de la $j$-\'esima  palabra
%codificada que ser\'a igual al tiempo en milisegundos entre
%codificaciones.
%
%Las variables $L_j$ ser\'an i.i.d. Si  $N(t)$ es el n\'umero de bits
%que entran en $t$ milisegundos es claro que $N(t)=t$.
%
%Sea $M(t)$=n\'umero de palabras codificadas en $t$ milisegundos
%(puede ser un n\'umero no entero), luego podemos poner
%$N(t)=\sum_{j=1}^{M(t)}=t$. Entonces la \emph{frecuencia} de
%palabras codificadas en $t$ milisegundos ser\'a $\frac{M(t)}{t}$ y
%por lo tanto aplicando el problema 28 b) tenemos que, con
%probabilidad $1$, la frecuencia de palabras codificadas a largo
%plazo es
%
%S $\lim_{t\to+\infty}\frac{M(t)}{t}=\frac{1}{E(L)}= \frac{1}{1
%0.1+ 2 0.09+ 3 0.081+4 (0.0729+0.6561) }=\frac{1}{3.439}=0.2908$
%palabras por milisegundo.
%
%
%
%c) Sea $S_j$=longitud de salida de la $j$-\'esima  palabra
%codificada.  Nos piden que calculemos $\lim_{t\to+\infty}
%\frac{\sum_{j=1}^{M(t)}S_j}{\sum_{j=1}^{M(t)} L_j}=
%\lim_{t\to+\infty}\frac{\sum_{j=1}^{M(t)}S_j}{t}$
%
%donde $\sum_{j=1}^{M(t)} L_j = t$ ya que el n\'umero total de bits 
%codificados en $t$ milisegundos es $t$.
%
%Finalmente, aplicando los resultados del problema 31b), obtenemos:
%
%$\lim_{t\to+\infty}\frac{\sum_{j=1}^{M(t)}S_j}{t}=\frac{E(S)}{E(L)}=
%\frac{3(1-0.6561)+1 0.6561}{3.439}=\frac{1.6878}{3.439}=0.49078$
%
%Luego, a largo plazo y con probabilidad $1$, la ratio de
%compresi\'on es del $49.078\%$
%
%



\end{document}
